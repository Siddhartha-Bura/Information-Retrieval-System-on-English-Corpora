{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-03T05:02:38.800630Z",
     "iopub.status.busy": "2022-03-03T05:02:38.798737Z",
     "iopub.status.idle": "2022-03-03T05:02:39.315673Z",
     "shell.execute_reply": "2022-03-03T05:02:39.315174Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk import flatten\n",
    "from collections import Counter\n",
    "from sys import argv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-03T05:02:39.322008Z",
     "iopub.status.busy": "2022-03-03T05:02:39.321395Z",
     "iopub.status.idle": "2022-03-03T05:02:39.322912Z",
     "shell.execute_reply": "2022-03-03T05:02:39.323199Z"
    }
   },
   "outputs": [],
   "source": [
    "import ntpath\n",
    "import pickle\n",
    "import ast\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-03T05:02:39.327241Z",
     "iopub.status.busy": "2022-03-03T05:02:39.326724Z",
     "iopub.status.idle": "2022-03-03T05:02:39.328706Z",
     "shell.execute_reply": "2022-03-03T05:02:39.328377Z"
    }
   },
   "outputs": [],
   "source": [
    "test_file = argv[1] # taking query file name through command line argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-03T05:02:39.335091Z",
     "iopub.status.busy": "2022-03-03T05:02:39.334679Z",
     "iopub.status.idle": "2022-03-03T05:02:39.337207Z",
     "shell.execute_reply": "2022-03-03T05:02:39.336851Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-f\n"
     ]
    }
   ],
   "source": [
    "print(test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions for Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-03T05:02:39.340350Z",
     "iopub.status.busy": "2022-03-03T05:02:39.339977Z",
     "iopub.status.idle": "2022-03-03T05:02:39.341958Z",
     "shell.execute_reply": "2022-03-03T05:02:39.341516Z"
    }
   },
   "outputs": [],
   "source": [
    "def porter_stemmer(x):  # using porter stemmer for stemming\n",
    "    if type(x) is not list:\n",
    "        x = x.split()\n",
    "    ps = PorterStemmer()\n",
    "    ans = [ps.stem(i) for i in x]\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-03T05:02:39.344581Z",
     "iopub.status.busy": "2022-03-03T05:02:39.344224Z",
     "iopub.status.idle": "2022-03-03T05:02:39.346225Z",
     "shell.execute_reply": "2022-03-03T05:02:39.345845Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_path_name(path): # utility function to extract name of the file from path\n",
    "    head, tail = ntpath.split(path)\n",
    "    return tail or ntpath.basename(head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-03T05:02:39.348903Z",
     "iopub.status.busy": "2022-03-03T05:02:39.348553Z",
     "iopub.status.idle": "2022-03-03T05:02:39.350238Z",
     "shell.execute_reply": "2022-03-03T05:02:39.349877Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_non_ascii(string): # Removes non ascii characters\n",
    "    return string.encode(\"ascii\", \"ignore\").decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-03T05:02:39.353236Z",
     "iopub.status.busy": "2022-03-03T05:02:39.352917Z",
     "iopub.status.idle": "2022-03-03T05:02:39.354792Z",
     "shell.execute_reply": "2022-03-03T05:02:39.354471Z"
    }
   },
   "outputs": [],
   "source": [
    "def scrub_special_characters(string,with_brackets = True,with_space = False):   # removes special characters from the string\n",
    "    replace_char = '@\"+-=\\\\_!#$%^.,&*()<>?/\\|}{~:;[]' if with_brackets else '@\"+-=\\\\_!#$%^.,&*<>?/\\|}{~:;[]'\n",
    "    join_char = ' ' if with_space else ''\n",
    "    return ''.join(x if not x in replace_char else join_char for x in string )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-03T05:02:39.358225Z",
     "iopub.status.busy": "2022-03-03T05:02:39.357907Z",
     "iopub.status.idle": "2022-03-03T05:02:39.359979Z",
     "shell.execute_reply": "2022-03-03T05:02:39.359536Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_stop_words(x,with_boolean_connectives=True): # Removes stop words\n",
    "    if type(x) is not list:\n",
    "        x = x.split()\n",
    "    stop_words = set(stopwords.words('english')) if with_boolean_connectives else set(stopwords.words('english'))-{'and','or','not'}\n",
    "    ans = [i for i in x if not i in stop_words]\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def white_space_tokenizer(string): # Tokenize the string\n",
    "    tk = WhitespaceTokenizer()\n",
    "    return tk.tokenize(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_preprocess(string, with_stop_words=False): # Preprocessing\n",
    "    cleaned_string = remove_non_ascii(string)  # removes non-ascii characters\n",
    "    cleaned_string= cleaned_string.replace(\"'s\",\" \")   \n",
    "    uncleaned_tokenized_list = white_space_tokenizer(cleaned_string) # tokenization\n",
    "    #uncleaned_tokenized_list = [i.lower() for i in uncleaned_tokenized_list]\n",
    "    uncleaned_tokenized_list = [scrub_special_characters(i)  for i in uncleaned_tokenized_list if scrub_special_characters(i)]\n",
    "    cleaned_token_list = remove_stop_words(uncleaned_tokenized_list) if not with_stop_words else uncleaned_tokenized_list\n",
    "    cleaned_token_list = [porter_stemmer(i) for i in cleaned_token_list if porter_stemmer(i)]  # case lower\n",
    "    return list(flatten(cleaned_token_list)) # put into list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading query data and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the queries\n",
    "query_file = open(test_file, 'r')\n",
    "queries = query_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperating the queryid and query\n",
    "queries_list=[]\n",
    "for query in queries:\n",
    "    queries_list.append(query.split(\"\\t\",1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing last element of the list if it contains only '/n'\n",
    "if len(queries_list[-1])==1:\n",
    "    queries_list=queries_list[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_queries={}\n",
    "for i in queries_list:\n",
    "    processed_queries[i[0]]=query_preprocess(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Q01': ['happen', 'amoghavarsha', 'rule'],\n",
       " 'Q02': ['caus', 'sadism'],\n",
       " 'Q03': ['zuckerberg', 'award', 'prize', 'encourag', 'research'],\n",
       " 'Q04': ['sunbird', 'owl', 'present', 'region'],\n",
       " 'Q05': ['rococo', 'architectur', 'design'],\n",
       " 'Q06': ['checksum', 'use', 'hash', 'function', 'cryptographi'],\n",
       " 'Q07': ['luke', 'skywalk', 'stori', 'movi', 'summari'],\n",
       " 'Q08': ['methylcobalamin', 'relat', 'vitamin'],\n",
       " 'Q09': ['rout', 'pondicherri', 'via', 'chengalpattu'],\n",
       " 'Q10': ['cacothelin', 'like', 'drug'],\n",
       " 'Q11': ['fort', 'templ', 'adilabad'],\n",
       " 'Q12': ['famou', 'biryani', 'hyderabad'],\n",
       " 'Q13': ['comput', 'game'],\n",
       " 'Q14': ['ahalya', 'indra', 'stori'],\n",
       " 'Q15': ['ravana', 'speak', 'prahasta', 'war'],\n",
       " 'Q16': ['product', 'matric', 'mathemat'],\n",
       " 'Q17': ['covid', 'vaccin'],\n",
       " 'Q18': ['genet', 'disord', 'mutat', 'due', 'uniparent', 'disomi'],\n",
       " 'Q19': ['jowar', 'grow'],\n",
       " 'Q20': ['vemulawada', 'shiva', 'templ', 'district']}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading all relevant dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = pickle.load(open('tf.p', \"rb\"))\n",
    "df = pickle.load(open('df.p', \"rb\"))\n",
    "idf = pickle.load(open('idf.p', \"rb\"))\n",
    "tf_idf = pickle.load(open('tf_idf.p', \"rb\"))\n",
    "idf_bm25 = pickle.load(open('idf_bm25.p', \"rb\"))\n",
    "doc_lens = pickle.load(open('doc_lens.p', \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boolean Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-03T05:02:39.372348Z",
     "iopub.status.busy": "2022-03-03T05:02:39.371968Z",
     "iopub.status.idle": "2022-03-03T05:02:39.373780Z",
     "shell.execute_reply": "2022-03-03T05:02:39.373446Z"
    }
   },
   "outputs": [],
   "source": [
    "def boolean_search(input_dir = \"english-corpora/\",count = 5):\n",
    "    output_dictionary_file_name = extract_path_name(input_dir)+\"_dictionary.p\"  # paths\n",
    "    output_postings_file_name = extract_path_name(input_dir)+\"_postings.txt\"\n",
    "    dictionary = pickle.load(open(output_dictionary_file_name, \"rb\"))\n",
    "    postings_file = open(output_postings_file_name, \"r\")\n",
    "    with open(test_file,'r') as query_file:\n",
    "        lines = query_file.readlines()\n",
    "    query_file.close()  \n",
    "    output = ''\n",
    "    for line in lines:\n",
    "        docs = set()\n",
    "        doc_score = {}\n",
    "        query_list = [p.strip() for p in line.split(None,1)]  \n",
    "        if len(query_list)==2:\n",
    "            query_id = query_list[0]\n",
    "            query = query_list[1]\n",
    "        else:\n",
    "            break   # if reached to end\n",
    "        tokens = query_preprocess(query,True)\n",
    "        tokens = list(set(tokens))\n",
    "        for token in tokens:\n",
    "            if token in dictionary:\n",
    "                byteOffset = dictionary[token][0]   # taking offset for postings list\n",
    "                pf = postings_file\n",
    "                pf.seek(byteOffset)\n",
    "                postList = ast.literal_eval(pf.readline().rstrip())\n",
    "                for i in postList:\n",
    "                    doc = i\n",
    "                    if doc in docs:\n",
    "                        doc_score[doc]+=1 # if already present add 1\n",
    "                    else:\n",
    "                        docs.add(doc)\n",
    "                        doc_score[doc]=1  # if not make the value 1\n",
    "        ans = [[k, v] for k, v in doc_score.items()] \n",
    "        ans.sort(key=lambda x: x[1],reverse=True)   # sort in descending order\n",
    "        ans = [x[0] for x in ans]\n",
    "        ans = ans[:count]\n",
    "        for i in ans:\n",
    "            output += query_id + \",1,\" + i + \",1\" + '\\n'\n",
    "    with open('boolean_qrel_output.txt', 'w+') as fh:\n",
    "        fh.write(output)\n",
    "    fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-03T05:02:39.515406Z",
     "iopub.status.busy": "2022-03-03T05:02:39.418915Z",
     "iopub.status.idle": "2022-03-03T05:02:39.815591Z",
     "shell.execute_reply": "2022-03-03T05:02:39.815270Z"
    }
   },
   "outputs": [],
   "source": [
    "boolean_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-03T05:02:39.825015Z",
     "iopub.status.busy": "2022-03-03T05:02:39.824311Z",
     "iopub.status.idle": "2022-03-03T05:02:39.826396Z",
     "shell.execute_reply": "2022-03-03T05:02:39.826092Z"
    }
   },
   "outputs": [],
   "source": [
    "def tfidf_search(count = 5):\n",
    "    \n",
    "    output = ''\n",
    "    for query_id in processed_queries:\n",
    "        query_tokens=processed_queries[query_id]\n",
    "        query_dictionary = {}       # query tokens as keys and its tfidf values as values to the dictionary\n",
    "        for k,v in Counter(query_tokens).items():   # calculating tfidf values for query given\n",
    "            if k in idf:\n",
    "                query_dictionary[k] = (v/len(query_tokens))*(idf[k])\n",
    "        ans = cosine_similarity(tf_idf,query_dictionary)\n",
    "        ans = ans[:count]\n",
    "        for i in ans:\n",
    "            output += query_id + \",1,\" + i + \",1\" + '\\n'\n",
    "    with open('tfidf_qrel_output.txt', 'w+') as fh:\n",
    "        fh.write(output)\n",
    "    fh.close()\n",
    "    \n",
    "    \n",
    "def cosine_similarity(dictionary,query_dictionary): # calculates cosine similarity iteratively between a set of document and the query as a document\n",
    "    denominator1 = math.sqrt(sum([query_dictionary[k]*query_dictionary[k] for k in query_dictionary.keys()]))\n",
    "    ans = []\n",
    "    for doc in dictionary.keys():\n",
    "        numerator = float(0.0)\n",
    "        for token in query_dictionary.keys():\n",
    "            if token in dictionary[doc].keys():\n",
    "                numerator += query_dictionary[token]*dictionary[doc][token]\n",
    "        denominator2 = math.sqrt(sum([dictionary[doc][k]*dictionary[doc][k] for k in dictionary[doc].keys()]))\n",
    "        ans.append([doc,numerator/(denominator1*denominator2)])\n",
    "    ans.sort(key=lambda x: x[1],reverse=True)\n",
    "    ans = [x[0] for x in ans]\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-03T05:02:39.829186Z",
     "iopub.status.busy": "2022-03-03T05:02:39.828662Z",
     "iopub.status.idle": "2022-03-03T05:02:41.953325Z",
     "shell.execute_reply": "2022-03-03T05:02:41.953613Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25 Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_search(count = 5):\n",
    "    b = 0.75\n",
    "    k_1 = 1.2\n",
    "    output = ''\n",
    "    avgdl=(sum(list(doc_lens.values())))/len(doc_lens)\n",
    "    for query_id in processed_queries:\n",
    "        bm25_scores=[]\n",
    "        query_tokens=processed_queries[query_id]\n",
    "        for doc in tf:\n",
    "            curr_score=0\n",
    "            for token in query_tokens :\n",
    "                if token in tf[doc]:\n",
    "                    curr_score+=(idf_bm25[token]*tf[doc][token]*doc_lens[doc]*(k_1+1))/((tf[doc][token]*doc_lens[doc])+k_1*(1-b+b*(doc_lens[doc]/avgdl)))\n",
    "            bm25_scores.append([curr_score,doc])\n",
    "        bm25_scores.sort(reverse=True) # sorting in descending order\n",
    "        doc_ids=[]\n",
    "        for i in range(count):  # returns top desired no of elements\n",
    "            doc_ids.append(bm25_scores[i][1])\n",
    "        for i in doc_ids:\n",
    "            output += query_id + \",1,\" + i + \",1\" + '\\n'\n",
    "    with open('bm25_qrel_output.txt', 'w+') as fh:\n",
    "        fh.write(output)\n",
    "    fh.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
