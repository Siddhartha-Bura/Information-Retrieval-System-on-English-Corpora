{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk import flatten\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import io\n",
    "import ntpath\n",
    "from nltk import flatten\n",
    "import math\n",
    "import pickle\n",
    "import ast\n",
    "import nltk\n",
    "import re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions for Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrub_special_characters(string,with_brackets = True,with_space = False):  # removes special characters from the string\n",
    "    replace_char = '@\"+-=\\\\_!#$%^.,&*()<>?/\\|}{~:;[]' if with_brackets else '@\"+-=\\\\_!#$%^.,&*<>?/\\|}{~:;[]'\n",
    "    join_char = ' ' if with_space else ''\n",
    "    return ''.join(x if not x in replace_char else join_char for x in string )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def white_space_tokenizer(string): # Tokenize the string\n",
    "    tk = WhitespaceTokenizer()\n",
    "    return tk.tokenize(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(x,with_boolean_connectives=True): # Removes stop words\n",
    "    if type(x) is not list:\n",
    "        x = x.split()\n",
    "    stop_words = set(stopwords.words('english')) if with_boolean_connectives else set(stopwords.words('english'))-{'and','or','not'}\n",
    "    ans = [i for i in x if not i in stop_words]\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def porter_stemmer(x):  # using porter stemmer for stemming\n",
    "    if type(x) is not list:\n",
    "        x = x.split()\n",
    "    ps = PorterStemmer()\n",
    "    ans = [ps.stem(i) for i in x]\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(string): # Removes non ascii characters\n",
    "    return string.encode(\"ascii\", \"ignore\").decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_path_name(path): # utility function to extract name of the file from path\n",
    "    head, tail = ntpath.split(path)\n",
    "    return tail or ntpath.basename(head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(string, with_stop_words=False): # Preprocessing\n",
    "    cleaned_string = remove_non_ascii(string)  # removes non-ascii characters\n",
    "    cleaned_string= cleaned_string.replace(\"'s\",\" \")   \n",
    "    cleaned_string = cleaned_string.replace(\"'\", \"\")\n",
    "    cleaned_string = cleaned_string.replace(\"`\", \"\")\n",
    "    uncleaned_tokenized_list = white_space_tokenizer(cleaned_string) # tokenization\n",
    "    #uncleaned_tokenized_list = [i.lower() for i in uncleaned_tokenized_list]\n",
    "    uncleaned_tokenized_list = [scrub_special_characters(i)  for i in uncleaned_tokenized_list if scrub_special_characters(i)]\n",
    "    cleaned_token_list = remove_stop_words(uncleaned_tokenized_list) if not with_stop_words else uncleaned_tokenized_list\n",
    "    cleaned_token_list = [porter_stemmer(i) for i in cleaned_token_list if porter_stemmer(i)]  # case lower\n",
    "    ans = list(flatten(cleaned_token_list)) # put into list\n",
    "    ans = [i  for i in ans if(len(i))>1]\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def core_preprocess(dictionary,input_dir = \"english-corpora/\"):\n",
    "    if (not os.path.exists(input_dir)): # check if path given is valid or not\n",
    "        print(\"file path incorrect!!!\")\n",
    "        sys.exit(1)\n",
    "    else:\n",
    "        doc_list = os.listdir(input_dir)  # list of all files within a directory\n",
    "        for doc in doc_list:\n",
    "            doc_obj = io.open(join(input_dir,str(doc)),'r',encoding='utf-8',errors='ignore') # open file  \n",
    "            text = doc_obj.read()\n",
    "            tokens = preprocess(text)\n",
    "            dictionary[doc] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc_dictionary = pickle.load(open('qurel_pickle.p', \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dictionary = {}\n",
    "core_preprocess(doc_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boolean Information Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Boolean_Information_Retrieval:\n",
    "    \n",
    "    dictionary = {} # dictionary with stemmed word as key with tuple as value containing offset and frequency\n",
    "    postings = {} # list of all documents that contains a word, retrieved using offset stored in dictionary\n",
    "    total_docs = [] # list of all documents\n",
    "\n",
    "    def __init__(self,input_dir = \"english-corpora/\"): # constructing output file paths with respect to input given\n",
    "        self.input_dir = input_dir\n",
    "        self.output_dictionary_file_name = extract_path_name(input_dir)+\"_dictionary.p\"\n",
    "        self.output_postings_file_name = extract_path_name(input_dir)+\"_postings.txt\"\n",
    "        \n",
    "    def generate_index(self): # build index\n",
    "        for doc in doc_dictionary:\n",
    "            self.total_docs.append(doc)\n",
    "            tokens = doc_dictionary[doc]\n",
    "            for token in tokens:\n",
    "                if token not in self.dictionary:\n",
    "                    self.dictionary[token] = (None, 1)\n",
    "                    self.postings[token] = [doc]\n",
    "                elif self.postings[token][self.dictionary[token][1]- 1] != doc: # checks if doc is already added to postings or not\n",
    "                    self.dictionary[token] = (None, self.dictionary[token][1] + 1)\n",
    "                    self.postings[token].append(doc)\n",
    "        self.total_docs = list(dict.fromkeys(self.total_docs))\n",
    "\n",
    "    # Write dictionary and index\n",
    "    def save_postings(self):\n",
    "        offset = 0\n",
    "        postings_file = open(self.output_postings_file_name, \"w\")\n",
    "        word_list = list(self.dictionary.keys())  # words in vocabulary\n",
    "        word_list.sort()                        # sort words\n",
    "        for token in word_list:\n",
    "            self.dictionary[token] = (offset,self.dictionary[token][1]) # storing offset in dictionary\n",
    "            postings_file.write(str(self.postings[token]) + '\\n') # writing documents list to postings file\n",
    "            offset = postings_file.tell()\n",
    "        postings_file.flush()\n",
    "    \n",
    "    def save_dictionary(self): # Dumps dictionary using pickle\n",
    "        dictionary_file = open(self.output_dictionary_file_name, \"wb\")\n",
    "        pickle.dump(self.dictionary, dictionary_file)\n",
    "        dictionary_file.flush()\n",
    "    \n",
    "    def clear(self):  # clears all variables\n",
    "        self.dictionary.clear()\n",
    "        self.postings.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bir = Boolean_Information_Retrieval()\n",
    "bir.generate_index()\n",
    "bir.save_postings()\n",
    "bir.save_dictionary()\n",
    "bir.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Information Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tf_Idf:\n",
    "    \n",
    "    tf = {} # dictionary with documents as keys and value as another dictionary i.e. token as a key and tf value corresponding to it as value\n",
    "    tf_idf={} # stores tfidf values\n",
    "    total_docs = []   # list of total unique documents \n",
    "    vocabulary = set() # set of all stemmed words from the entire corpus\n",
    "    inverse_document_freq = {}  # idf as dictionary for each token as key\n",
    "    df={} # document frequency\n",
    "\n",
    "    def __init__(self,input_dir = \"english-corpora/\"): # constructing output file paths with respect to input given\n",
    "        self.input_dir = input_dir\n",
    "        self.output_dictionary_file_name = extract_path_name(input_dir)+\"_tfidf_dictionary.p\"\n",
    "        self.output_idf_file_name = extract_path_name(input_dir)+\"_idf_dictionary.p\"\n",
    "        \n",
    "    def generate_index(self):  # build index          \n",
    "        for doc in doc_dictionary:\n",
    "            self.total_docs.append(doc)\n",
    "            tokens = doc_dictionary[doc]\n",
    "            self.vocabulary.update(tokens)  # update the vocabulary\n",
    "            self.tf[doc] = self.cal_tf(tokens)     # calculates only tf values in a document w.r.t each token\n",
    "        self.total_docs = list(dict.fromkeys(self.total_docs))  # making total documents list unique\n",
    "        self.idf()                # calculate idf value for each token\n",
    "        self.tf_idf_calculator()  # calculates tfidf values in each document, or simply multiples already existing tf values with idf values\n",
    "    \n",
    "    def cal_tf(self,tokens):  # Term Frequency calculator\n",
    "        dictionary = {}\n",
    "        for k,v in Counter(tokens).items():\n",
    "            dictionary[k] = (v/len(tokens))   # Normalizing w.r.t length of the document\n",
    "            #dictionary[k] = v\n",
    "        for term, _ in dictionary.items():\n",
    "            if term in self.df.keys():\n",
    "                self.df[term]+=1\n",
    "            else:\n",
    "                self.df[term]=1\n",
    "        return dictionary\n",
    "    \n",
    "    def idf(self):        # Inverse Document Frequency calculator\n",
    "        for i in self.df.keys():\n",
    "            self.inverse_document_freq[i] = math.log(len(self.total_docs)/(self.df[i] + 1))\n",
    "            \n",
    "    def tf_idf_calculator(self): #tf_idf calculator\n",
    "        for doc in self.tf.keys():\n",
    "            temp_tf_idf={}\n",
    "            doc_len=sum(list(self.tf[doc].values()))\n",
    "            for token in self.tf[doc].keys():\n",
    "                temp_tf_idf[token] = self.inverse_document_freq[token]*(self.tf[doc][token]/doc_len)# Multiplies idf with already existing term frequency\n",
    "            self.tf_idf[doc]=temp_tf_idf\n",
    "            \n",
    "    def save_dictionary(self):  # Dumps dictionary using pickle\n",
    "        dictionary_file = open('tf.p', \"wb\")\n",
    "        pickle.dump(self.tf, dictionary_file)\n",
    "        dictionary_file.flush()\n",
    "        \n",
    "        df_file = open('df.p', \"wb\")\n",
    "        pickle.dump(self.df, df_file)\n",
    "        df_file.flush()\n",
    "        \n",
    "        df_file = open('tf_idf.p', \"wb\")\n",
    "        pickle.dump(self.tf_idf, df_file)\n",
    "        df_file.flush()\n",
    "    \n",
    "        idf_file = open('idf.p', \"wb\")\n",
    "        pickle.dump(self.inverse_document_freq, idf_file)\n",
    "        idf_file.flush()\n",
    "        \n",
    "    def clear(self):  # clears all variables\n",
    "        self.tf.clear()\n",
    "        self.tf_idf.clear() \n",
    "        self.total_docs.clear()\n",
    "        self.vocabulary.clear() \n",
    "        self.inverse_document_freq.clear()\n",
    "        self.df.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = Tf_Idf()\n",
    "tf.generate_index()\n",
    "tf.save_dictionary()\n",
    "tf.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25 Information Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25:\n",
    "    \n",
    "    dictionary = {}  # documents as keys and value is a dictionary with token as key and corresponding term frequency as value\n",
    "    total_docs = []  # list of total unique documents \n",
    "    vocabulary = set() # set of all stemmed words from the entire corpus\n",
    "    inverse_document_freq = {} # idf as dictionary for each token as key\n",
    "    tf= pickle.load(open('tf.p', \"rb\"))\n",
    "    df= pickle.load(open('df.p', \"rb\"))\n",
    "    doc_lens={}\n",
    "    \n",
    "    def __init__(self,input_dir = \"english-corpora/\"): # constructing output file paths with respect to input given\n",
    "        self.input_dir = input_dir\n",
    "        self.output_dictionary_file_name = extract_path_name(input_dir)+\"_bm25_dictionary.p\"\n",
    "        self.output_idf_file_name = extract_path_name(input_dir)+\"_bm25_idf_dictionary.p\"\n",
    "        \n",
    "    def generate_index(self):  # build index\n",
    "        \n",
    "        for doc in doc_dictionary:\n",
    "            tokens = doc_dictionary[doc]           \n",
    "            self.doc_lens[doc]=len(tokens)\n",
    "        self.idf()\n",
    "    \n",
    "    def idf(self): \n",
    "        # Inverse Document Frequency calculator\n",
    "        for i in self.df.keys():\n",
    "            self.inverse_document_freq[i] = math.log(((len(doc_dictionary) - self.df[i] + 0.5)/(self.df[i] + 0.5)) + 1)\n",
    "                \n",
    "    def save_idf(self):           # Dumps idf dictionary and document lengths dictionary using pickle\n",
    "        idf_file = open('idf_bm25.p', \"wb\")\n",
    "        pickle.dump(self.inverse_document_freq, idf_file)\n",
    "        idf_file.flush()\n",
    "        \n",
    "        idf_file = open('doc_lens.p', \"wb\")\n",
    "        pickle.dump(self.doc_lens, idf_file)\n",
    "        idf_file.flush()\n",
    "    \n",
    "    def clear(self):             # clears all variables\n",
    "        self.dictionary.clear()\n",
    "        self.total_docs.clear()\n",
    "        self.vocabulary.clear()\n",
    "        self.inverse_document_freq.clear()\n",
    "        self.tf.clear()\n",
    "        self.df.clear()\n",
    "        self.doc_lens.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm = BM25()\n",
    "bm.generate_index()\n",
    "bm.save_idf()\n",
    "bm.clear()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
